# 1. Seq2Seq with attention, Encoder-decoder architecture, Attention mechanism


## seq2seq model

- sequence to sequence(many to many mapping)
  - machine translation
- it takes a sequence of words as input and gives a sequence of words as output
- it composed of an encoder and a decoder

## Attention is Great!
- attention significantly improves NMT performance
- attention solves the bottleneck problem
- attention helps with vanishing gradient problem
- attention provides some interpretability

## Attention Examples in Machine Translation