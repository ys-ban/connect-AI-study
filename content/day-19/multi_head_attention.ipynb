{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multi_head_attention.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMNJA9pRX0mR"
      },
      "source": [
        "# Multi-head Attention\n",
        "\n",
        "1. multi-head attention 및 self-attetion 구현\n",
        "2. 각 과정에서 일어나는 연산과 input/output형태 이해\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYzATTqKYHpx"
      },
      "source": [
        "## 필요 패키지 import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1i0d_VOYKZt"
      },
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f1uu9rAYSvO"
      },
      "source": [
        "## 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae20bpFcYY-o"
      },
      "source": [
        "pad_id = 0\n",
        "vocab_size = 100\n",
        "\n",
        "data = [\n",
        "  [62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75],\n",
        "  [60, 96, 51, 32, 90],\n",
        "  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54],\n",
        "  [75, 51],\n",
        "  [66, 88, 98, 47],\n",
        "  [21, 39, 10, 64, 21],\n",
        "  [98],\n",
        "  [77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10],\n",
        "  [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34],\n",
        "  [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43]\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho8yXX6GYb4y"
      },
      "source": [
        "def padding(data):\n",
        "  max_len = len(max(data, key = len))\n",
        "  print(f\"Maximum length: {max_len}\")\n",
        "\n",
        "  for i, seq in enumerate(tqdm(data)):\n",
        "    if len(seq)<max_len:\n",
        "      data[i] = seq + [pad_id]*(max_len-len(seq))\n",
        "    \n",
        "  return data, max_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "My8FvguKY-KX",
        "outputId": "4cb58e63-acfe-44d0-afe3-5041755ae37e"
      },
      "source": [
        "data, max_len = padding(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 49991.70it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Maximum length: 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-3b5dQaZEIV",
        "outputId": "0bcfc86b-67ae-480e-a1b4-319d4c1c9f17"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75, 0, 0, 0, 0],\n",
              " [60, 96, 51, 32, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [75, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [66, 88, 98, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [21, 39, 10, 64, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [98, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [77,\n",
              "  65,\n",
              "  51,\n",
              "  77,\n",
              "  19,\n",
              "  15,\n",
              "  35,\n",
              "  19,\n",
              "  23,\n",
              "  97,\n",
              "  50,\n",
              "  46,\n",
              "  53,\n",
              "  42,\n",
              "  45,\n",
              "  91,\n",
              "  66,\n",
              "  3,\n",
              "  43,\n",
              "  10],\n",
              " [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34, 0, 0, 0, 0],\n",
              " [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04x7frA_ZEm5"
      },
      "source": [
        "## Hyperparameter 세팅 및 embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "askewtO-ZLcS"
      },
      "source": [
        "d_model = 512 # hidden size of model\n",
        "num_heads = 8 # the number of heads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y49i-dWZXQ2"
      },
      "source": [
        "embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "# B: batch size, L: maximum sequence length\n",
        "batch = torch.LongTensor(data)  # (B, L)\n",
        "batch_emb = embedding(batch)    # (B, L, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5leY-iMZsvI",
        "outputId": "15158961-044d-47bf-80e8-50cd2c122699"
      },
      "source": [
        "print(batch_emb)\n",
        "print(batch_emb.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.8649, -1.7624, -0.1467,  ...,  0.1065, -0.5493, -0.9687],\n",
            "         [-0.5280,  0.5591,  0.3295,  ...,  0.9948, -2.4267, -1.2439],\n",
            "         [-0.9900,  1.9719,  0.3138,  ...,  0.0512, -1.3343,  1.2161],\n",
            "         ...,\n",
            "         [ 0.0243,  0.3623,  0.3413,  ..., -0.2686, -0.3003,  1.2229],\n",
            "         [ 0.0243,  0.3623,  0.3413,  ..., -0.2686, -0.3003,  1.2229],\n",
            "         [ 0.0243,  0.3623,  0.3413,  ..., -0.2686, -0.3003,  1.2229]],\n",
            "\n",
            "        [[-0.7190,  0.5153, -1.0311,  ..., -1.0616,  1.0147,  0.2931],\n",
            "         [ 1.0068,  0.0157, -0.0789,  ..., -0.1688,  0.6181,  0.2007],\n",
            "         [-0.1904, -0.7859, -0.5010,  ..., -1.7233,  0.1630,  0.3240],\n",
            "         ...,\n",
            "         [ 0.0243,  0.3623,  0.3413,  ..., -0.2686, -0.3003,  1.2229],\n",
            "         [ 0.0243,  0.3623,  0.3413,  ..., -0.2686, -0.3003,  1.2229],\n",
            "         [ 0.0243,  0.3623,  0.3413,  ..., -0.2686, -0.3003,  1.2229]],\n",
            "\n",
            "        [[-0.3793, -0.6177,  1.3900,  ...,  0.2882,  0.2737,  1.1043],\n",
            "         [ 1.5289, -2.1897,  0.4747,  ..., -0.3121, -0.8938,  0.4479],\n",
            "         [ 1.2728,  0.4169, -0.6850,  ..., -0.2347, -0.4637,  1.1379],\n",
            "         ...,\n",
            "         [ 0.0243,  0.3623,  0.3413,  ..., -0.2686, -0.3003,  1.2229],\n",
            "         [ 0.0243,  0.3623,  0.3413,  ..., -0.2686, -0.3003,  1.2229],\n",
            "         [ 0.0243,  0.3623,  0.3413,  ..., -0.2686, -0.3003,  1.2229]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.0608,  0.2740, -1.9550,  ..., -0.0971,  0.4902, -0.2943],\n",
            "         [ 0.1697, -0.5969, -0.4035,  ..., -0.9936,  0.1988,  1.7707],\n",
            "         [-0.1904, -0.7859, -0.5010,  ..., -1.7233,  0.1630,  0.3240],\n",
            "         ...,\n",
            "         [ 0.6385,  0.9908,  0.9425,  ..., -0.1885, -0.0692,  2.1704],\n",
            "         [-0.8145,  0.2825,  1.7902,  ..., -0.2035, -0.2727,  1.2951],\n",
            "         [ 1.9248, -0.9291, -1.1720,  ...,  0.1079, -0.6275,  0.8215]],\n",
            "\n",
            "        [[ 1.5151, -1.5735,  0.1933,  ...,  0.0594,  1.7169, -1.1091],\n",
            "         [ 0.8341,  1.3902, -0.2519,  ...,  0.0450,  0.1259, -1.4140],\n",
            "         [-1.6981, -0.6546,  0.3720,  ..., -0.4854,  0.1680,  2.0320],\n",
            "         ...,\n",
            "         [ 0.0243,  0.3623,  0.3413,  ..., -0.2686, -0.3003,  1.2229],\n",
            "         [ 0.0243,  0.3623,  0.3413,  ..., -0.2686, -0.3003,  1.2229],\n",
            "         [ 0.0243,  0.3623,  0.3413,  ..., -0.2686, -0.3003,  1.2229]],\n",
            "\n",
            "        [[ 1.7701, -0.7155,  0.7989,  ...,  0.1895,  1.6819,  0.0849],\n",
            "         [ 0.8341,  1.3902, -0.2519,  ...,  0.0450,  0.1259, -1.4140],\n",
            "         [-0.2627,  1.1936,  2.4442,  ..., -0.5607,  0.4391,  0.7516],\n",
            "         ...,\n",
            "         [ 0.0243,  0.3623,  0.3413,  ..., -0.2686, -0.3003,  1.2229],\n",
            "         [ 0.0243,  0.3623,  0.3413,  ..., -0.2686, -0.3003,  1.2229],\n",
            "         [ 0.0243,  0.3623,  0.3413,  ..., -0.2686, -0.3003,  1.2229]]],\n",
            "       grad_fn=<EmbeddingBackward>)\n",
            "torch.Size([10, 20, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDuFYtLHZ0Yv"
      },
      "source": [
        "## Linear transformation & split into several heads\n",
        "\n",
        "define the matrices used in Multi-head attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FO_aTvDcaHcS"
      },
      "source": [
        "w_q = nn.Linear(d_model, d_model)\n",
        "w_k = nn.Linear(d_model, d_model)\n",
        "w_v = nn.Linear(d_model, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2X2DIIdaUrB"
      },
      "source": [
        "w_0 = nn.Linear(d_model, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYPVwZYFaXk1",
        "outputId": "527b552e-ab94-428c-d7e6-d37c6e6c4ca0"
      },
      "source": [
        "q = w_q(batch_emb)  # (B, L, d_model)\n",
        "k = w_k(batch_emb)  # (B, L, d_model)\n",
        "v = w_v(batch_emb)  # (B, L, d_model)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 20, 512])\n",
            "torch.Size([10, 20, 512])\n",
            "torch.Size([10, 20, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYss-lchagoO"
      },
      "source": [
        "make vectors for `Q, k, v` divided into the number of the dimension of `num_head`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7hiF8t9a_Ha",
        "outputId": "97f15c74-1b39-490f-c581-940270036bd8"
      },
      "source": [
        "batch_size = q.shape[0]\n",
        "d_k = d_model // num_heads\n",
        "\n",
        "q = q.view(batch_size, -1, num_heads, d_k)\n",
        "k = k.view(batch_size, -1, num_heads, d_k)\n",
        "v = v.view(batch_size, -1, num_heads, d_k)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 20, 8, 64])\n",
            "torch.Size([10, 20, 8, 64])\n",
            "torch.Size([10, 20, 8, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVC5kabEbR1N"
      },
      "source": [
        "## Scaled-dot product self-attention\n",
        "\n",
        "it is the process of self-attnetion for each head"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8XIIhIybs8L",
        "outputId": "9f271abe-0b2b-4631-f1e6-3c560819d3a5"
      },
      "source": [
        "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "attn_dists = F.softmax(attn_scores, dim = -1)\n",
        "\n",
        "print(attn_dists)\n",
        "print(attn_dists.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[0.1318, 0.1550, 0.0935,  ..., 0.0809, 0.1775, 0.1748],\n",
            "          [0.2706, 0.0917, 0.1166,  ..., 0.1883, 0.1077, 0.0368],\n",
            "          [0.1419, 0.1564, 0.1201,  ..., 0.1719, 0.0831, 0.1145],\n",
            "          ...,\n",
            "          [0.0702, 0.2032, 0.1900,  ..., 0.0995, 0.1312, 0.0840],\n",
            "          [0.1302, 0.1279, 0.1387,  ..., 0.1376, 0.1013, 0.0941],\n",
            "          [0.1226, 0.0985, 0.1069,  ..., 0.1747, 0.1171, 0.1145]],\n",
            "\n",
            "         [[0.1224, 0.1282, 0.1021,  ..., 0.1567, 0.2502, 0.0892],\n",
            "          [0.0843, 0.1336, 0.1431,  ..., 0.0916, 0.1909, 0.1151],\n",
            "          [0.1078, 0.1030, 0.0970,  ..., 0.1682, 0.1435, 0.1183],\n",
            "          ...,\n",
            "          [0.1431, 0.1403, 0.1357,  ..., 0.0652, 0.0978, 0.1330],\n",
            "          [0.0899, 0.0785, 0.2660,  ..., 0.0821, 0.0997, 0.1342],\n",
            "          [0.1331, 0.0730, 0.0848,  ..., 0.1372, 0.0731, 0.2091]],\n",
            "\n",
            "         [[0.1150, 0.1244, 0.1434,  ..., 0.1598, 0.1230, 0.1073],\n",
            "          [0.1735, 0.1504, 0.1092,  ..., 0.0756, 0.1283, 0.0850],\n",
            "          [0.1940, 0.1704, 0.0866,  ..., 0.0853, 0.0742, 0.0985],\n",
            "          ...,\n",
            "          [0.0783, 0.1664, 0.1051,  ..., 0.1117, 0.1251, 0.0801],\n",
            "          [0.0606, 0.1248, 0.1026,  ..., 0.1442, 0.1654, 0.0996],\n",
            "          [0.2463, 0.0567, 0.0768,  ..., 0.1381, 0.1888, 0.1088]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0883, 0.0875, 0.0924,  ..., 0.1407, 0.1071, 0.1081],\n",
            "          [0.0714, 0.0823, 0.0752,  ..., 0.1351, 0.1445, 0.1844],\n",
            "          [0.1050, 0.1974, 0.1106,  ..., 0.0571, 0.1099, 0.1213],\n",
            "          ...,\n",
            "          [0.1108, 0.1247, 0.1002,  ..., 0.1505, 0.1915, 0.1933],\n",
            "          [0.1155, 0.1108, 0.1455,  ..., 0.1601, 0.1204, 0.0946],\n",
            "          [0.0750, 0.1196, 0.1080,  ..., 0.0879, 0.2136, 0.0884]],\n",
            "\n",
            "         [[0.0883, 0.0875, 0.0924,  ..., 0.1407, 0.1071, 0.1081],\n",
            "          [0.0714, 0.0823, 0.0752,  ..., 0.1351, 0.1445, 0.1844],\n",
            "          [0.1050, 0.1974, 0.1106,  ..., 0.0571, 0.1099, 0.1213],\n",
            "          ...,\n",
            "          [0.1108, 0.1247, 0.1002,  ..., 0.1505, 0.1915, 0.1933],\n",
            "          [0.1155, 0.1108, 0.1455,  ..., 0.1601, 0.1204, 0.0946],\n",
            "          [0.0750, 0.1196, 0.1080,  ..., 0.0879, 0.2136, 0.0884]],\n",
            "\n",
            "         [[0.0883, 0.0875, 0.0924,  ..., 0.1407, 0.1071, 0.1081],\n",
            "          [0.0714, 0.0823, 0.0752,  ..., 0.1351, 0.1445, 0.1844],\n",
            "          [0.1050, 0.1974, 0.1106,  ..., 0.0571, 0.1099, 0.1213],\n",
            "          ...,\n",
            "          [0.1108, 0.1247, 0.1002,  ..., 0.1505, 0.1915, 0.1933],\n",
            "          [0.1155, 0.1108, 0.1455,  ..., 0.1601, 0.1204, 0.0946],\n",
            "          [0.0750, 0.1196, 0.1080,  ..., 0.0879, 0.2136, 0.0884]]],\n",
            "\n",
            "\n",
            "        [[[0.2004, 0.1152, 0.0743,  ..., 0.1695, 0.1447, 0.1430],\n",
            "          [0.1739, 0.1692, 0.1747,  ..., 0.1007, 0.1030, 0.0762],\n",
            "          [0.1217, 0.1233, 0.1358,  ..., 0.1132, 0.2140, 0.0762],\n",
            "          ...,\n",
            "          [0.1200, 0.1490, 0.0843,  ..., 0.1613, 0.1130, 0.1339],\n",
            "          [0.1187, 0.1426, 0.1393,  ..., 0.1261, 0.1086, 0.0782],\n",
            "          [0.2190, 0.1255, 0.1397,  ..., 0.1720, 0.0692, 0.0828]],\n",
            "\n",
            "         [[0.1945, 0.1117, 0.1655,  ..., 0.1507, 0.0851, 0.0762],\n",
            "          [0.0919, 0.1065, 0.1261,  ..., 0.1386, 0.0974, 0.0958],\n",
            "          [0.1395, 0.1129, 0.1081,  ..., 0.1422, 0.0659, 0.1675],\n",
            "          ...,\n",
            "          [0.1301, 0.1219, 0.1303,  ..., 0.1570, 0.1061, 0.1024],\n",
            "          [0.0811, 0.2355, 0.1179,  ..., 0.1651, 0.0640, 0.1926],\n",
            "          [0.1027, 0.1606, 0.1330,  ..., 0.0639, 0.2231, 0.0655]],\n",
            "\n",
            "         [[0.1070, 0.1349, 0.1827,  ..., 0.0851, 0.1461, 0.1071],\n",
            "          [0.2067, 0.0647, 0.2142,  ..., 0.0954, 0.1062, 0.0500],\n",
            "          [0.1546, 0.1263, 0.0647,  ..., 0.0817, 0.1976, 0.0946],\n",
            "          ...,\n",
            "          [0.0767, 0.1043, 0.1402,  ..., 0.2305, 0.1006, 0.1401],\n",
            "          [0.1166, 0.1861, 0.1140,  ..., 0.1110, 0.1453, 0.0839],\n",
            "          [0.1037, 0.0865, 0.1516,  ..., 0.1727, 0.0920, 0.0887]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0883, 0.0875, 0.0924,  ..., 0.1407, 0.1071, 0.1081],\n",
            "          [0.0714, 0.0823, 0.0752,  ..., 0.1351, 0.1445, 0.1844],\n",
            "          [0.1050, 0.1974, 0.1106,  ..., 0.0571, 0.1099, 0.1213],\n",
            "          ...,\n",
            "          [0.1108, 0.1247, 0.1002,  ..., 0.1505, 0.1915, 0.1933],\n",
            "          [0.1155, 0.1108, 0.1455,  ..., 0.1601, 0.1204, 0.0946],\n",
            "          [0.0750, 0.1196, 0.1080,  ..., 0.0879, 0.2136, 0.0884]],\n",
            "\n",
            "         [[0.0883, 0.0875, 0.0924,  ..., 0.1407, 0.1071, 0.1081],\n",
            "          [0.0714, 0.0823, 0.0752,  ..., 0.1351, 0.1445, 0.1844],\n",
            "          [0.1050, 0.1974, 0.1106,  ..., 0.0571, 0.1099, 0.1213],\n",
            "          ...,\n",
            "          [0.1108, 0.1247, 0.1002,  ..., 0.1505, 0.1915, 0.1933],\n",
            "          [0.1155, 0.1108, 0.1455,  ..., 0.1601, 0.1204, 0.0946],\n",
            "          [0.0750, 0.1196, 0.1080,  ..., 0.0879, 0.2136, 0.0884]],\n",
            "\n",
            "         [[0.0883, 0.0875, 0.0924,  ..., 0.1407, 0.1071, 0.1081],\n",
            "          [0.0714, 0.0823, 0.0752,  ..., 0.1351, 0.1445, 0.1844],\n",
            "          [0.1050, 0.1974, 0.1106,  ..., 0.0571, 0.1099, 0.1213],\n",
            "          ...,\n",
            "          [0.1108, 0.1247, 0.1002,  ..., 0.1505, 0.1915, 0.1933],\n",
            "          [0.1155, 0.1108, 0.1455,  ..., 0.1601, 0.1204, 0.0946],\n",
            "          [0.0750, 0.1196, 0.1080,  ..., 0.0879, 0.2136, 0.0884]]],\n",
            "\n",
            "\n",
            "        [[[0.0920, 0.1154, 0.1235,  ..., 0.1310, 0.1588, 0.1482],\n",
            "          [0.0849, 0.1943, 0.1621,  ..., 0.1294, 0.1214, 0.0968],\n",
            "          [0.1265, 0.1187, 0.1468,  ..., 0.0616, 0.1062, 0.1970],\n",
            "          ...,\n",
            "          [0.1786, 0.1063, 0.0796,  ..., 0.2017, 0.1197, 0.1281],\n",
            "          [0.0834, 0.2137, 0.1123,  ..., 0.0787, 0.1132, 0.1801],\n",
            "          [0.0901, 0.1177, 0.0962,  ..., 0.1291, 0.1255, 0.0965]],\n",
            "\n",
            "         [[0.1028, 0.1042, 0.1822,  ..., 0.1801, 0.0871, 0.0997],\n",
            "          [0.1255, 0.1097, 0.1102,  ..., 0.1244, 0.1611, 0.1403],\n",
            "          [0.0830, 0.1492, 0.0874,  ..., 0.1237, 0.1248, 0.1981],\n",
            "          ...,\n",
            "          [0.1321, 0.1348, 0.1236,  ..., 0.1592, 0.0548, 0.0915],\n",
            "          [0.0858, 0.1234, 0.1065,  ..., 0.0632, 0.1932, 0.2020],\n",
            "          [0.1488, 0.1178, 0.0996,  ..., 0.1330, 0.0944, 0.1054]],\n",
            "\n",
            "         [[0.1991, 0.1178, 0.0992,  ..., 0.0898, 0.1159, 0.1080],\n",
            "          [0.0771, 0.1757, 0.1168,  ..., 0.1568, 0.1491, 0.1074],\n",
            "          [0.2051, 0.1288, 0.1021,  ..., 0.0593, 0.1673, 0.1314],\n",
            "          ...,\n",
            "          [0.0887, 0.1171, 0.1695,  ..., 0.0863, 0.1304, 0.1531],\n",
            "          [0.0976, 0.2020, 0.0899,  ..., 0.0796, 0.1385, 0.1096],\n",
            "          [0.1032, 0.1653, 0.0869,  ..., 0.1260, 0.0943, 0.1498]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0883, 0.0875, 0.0924,  ..., 0.1407, 0.1071, 0.1081],\n",
            "          [0.0714, 0.0823, 0.0752,  ..., 0.1351, 0.1445, 0.1844],\n",
            "          [0.1050, 0.1974, 0.1106,  ..., 0.0571, 0.1099, 0.1213],\n",
            "          ...,\n",
            "          [0.1108, 0.1247, 0.1002,  ..., 0.1505, 0.1915, 0.1933],\n",
            "          [0.1155, 0.1108, 0.1455,  ..., 0.1601, 0.1204, 0.0946],\n",
            "          [0.0750, 0.1196, 0.1080,  ..., 0.0879, 0.2136, 0.0884]],\n",
            "\n",
            "         [[0.0883, 0.0875, 0.0924,  ..., 0.1407, 0.1071, 0.1081],\n",
            "          [0.0714, 0.0823, 0.0752,  ..., 0.1351, 0.1445, 0.1844],\n",
            "          [0.1050, 0.1974, 0.1106,  ..., 0.0571, 0.1099, 0.1213],\n",
            "          ...,\n",
            "          [0.1108, 0.1247, 0.1002,  ..., 0.1505, 0.1915, 0.1933],\n",
            "          [0.1155, 0.1108, 0.1455,  ..., 0.1601, 0.1204, 0.0946],\n",
            "          [0.0750, 0.1196, 0.1080,  ..., 0.0879, 0.2136, 0.0884]],\n",
            "\n",
            "         [[0.0883, 0.0875, 0.0924,  ..., 0.1407, 0.1071, 0.1081],\n",
            "          [0.0714, 0.0823, 0.0752,  ..., 0.1351, 0.1445, 0.1844],\n",
            "          [0.1050, 0.1974, 0.1106,  ..., 0.0571, 0.1099, 0.1213],\n",
            "          ...,\n",
            "          [0.1108, 0.1247, 0.1002,  ..., 0.1505, 0.1915, 0.1933],\n",
            "          [0.1155, 0.1108, 0.1455,  ..., 0.1601, 0.1204, 0.0946],\n",
            "          [0.0750, 0.1196, 0.1080,  ..., 0.0879, 0.2136, 0.0884]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0.0740, 0.1788, 0.1454,  ..., 0.0937, 0.1314, 0.0661],\n",
            "          [0.1308, 0.0853, 0.0691,  ..., 0.1572, 0.1807, 0.2149],\n",
            "          [0.1197, 0.1688, 0.1264,  ..., 0.1313, 0.1202, 0.0979],\n",
            "          ...,\n",
            "          [0.1118, 0.1084, 0.2141,  ..., 0.1258, 0.1166, 0.0837],\n",
            "          [0.1622, 0.2179, 0.1082,  ..., 0.1248, 0.0958, 0.1242],\n",
            "          [0.1281, 0.1131, 0.1528,  ..., 0.0855, 0.1725, 0.1160]],\n",
            "\n",
            "         [[0.0984, 0.0650, 0.1124,  ..., 0.0850, 0.1453, 0.1562],\n",
            "          [0.1112, 0.1101, 0.1126,  ..., 0.0879, 0.1527, 0.1554],\n",
            "          [0.1233, 0.1588, 0.2108,  ..., 0.1096, 0.0721, 0.1180],\n",
            "          ...,\n",
            "          [0.1056, 0.1382, 0.0746,  ..., 0.0941, 0.1745, 0.1440],\n",
            "          [0.0642, 0.0735, 0.1991,  ..., 0.1685, 0.1527, 0.1105],\n",
            "          [0.0525, 0.1238, 0.2781,  ..., 0.0632, 0.0897, 0.1739]],\n",
            "\n",
            "         [[0.1070, 0.1349, 0.1827,  ..., 0.0851, 0.1461, 0.1071],\n",
            "          [0.2067, 0.0647, 0.2142,  ..., 0.0954, 0.1062, 0.0500],\n",
            "          [0.1546, 0.1263, 0.0647,  ..., 0.0817, 0.1976, 0.0946],\n",
            "          ...,\n",
            "          [0.0767, 0.1043, 0.1402,  ..., 0.2305, 0.1006, 0.1401],\n",
            "          [0.1166, 0.1861, 0.1140,  ..., 0.1110, 0.1453, 0.0839],\n",
            "          [0.1037, 0.0865, 0.1516,  ..., 0.1727, 0.0920, 0.0887]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.1081, 0.1655, 0.0901,  ..., 0.1421, 0.0487, 0.1567],\n",
            "          [0.1381, 0.0787, 0.0813,  ..., 0.1242, 0.1114, 0.1171],\n",
            "          [0.2091, 0.1531, 0.0909,  ..., 0.1072, 0.1158, 0.0736],\n",
            "          ...,\n",
            "          [0.1253, 0.1369, 0.2015,  ..., 0.0863, 0.0947, 0.1357],\n",
            "          [0.1424, 0.1548, 0.1204,  ..., 0.2610, 0.1055, 0.0636],\n",
            "          [0.1016, 0.1107, 0.1471,  ..., 0.1630, 0.1394, 0.0833]],\n",
            "\n",
            "         [[0.1547, 0.1741, 0.1336,  ..., 0.1052, 0.1094, 0.0859],\n",
            "          [0.1535, 0.1685, 0.0888,  ..., 0.1401, 0.1085, 0.1601],\n",
            "          [0.1106, 0.1245, 0.1128,  ..., 0.1593, 0.0796, 0.1430],\n",
            "          ...,\n",
            "          [0.1012, 0.1709, 0.0701,  ..., 0.2293, 0.0599, 0.1393],\n",
            "          [0.1788, 0.1044, 0.1772,  ..., 0.1439, 0.1172, 0.0645],\n",
            "          [0.0814, 0.1806, 0.2213,  ..., 0.0414, 0.1182, 0.0989]],\n",
            "\n",
            "         [[0.1452, 0.0898, 0.1924,  ..., 0.0665, 0.1020, 0.0937],\n",
            "          [0.0586, 0.1380, 0.0874,  ..., 0.1458, 0.1536, 0.1033],\n",
            "          [0.1589, 0.1467, 0.0839,  ..., 0.0946, 0.2025, 0.0683],\n",
            "          ...,\n",
            "          [0.1730, 0.1210, 0.1599,  ..., 0.0983, 0.0767, 0.1576],\n",
            "          [0.1300, 0.1147, 0.0685,  ..., 0.1585, 0.0977, 0.1049],\n",
            "          [0.1374, 0.0605, 0.1187,  ..., 0.1527, 0.1006, 0.0647]]],\n",
            "\n",
            "\n",
            "        [[[0.1015, 0.0793, 0.1244,  ..., 0.2385, 0.0644, 0.1366],\n",
            "          [0.1934, 0.1589, 0.1140,  ..., 0.1162, 0.1504, 0.0941],\n",
            "          [0.1133, 0.1215, 0.1334,  ..., 0.1339, 0.1491, 0.0800],\n",
            "          ...,\n",
            "          [0.1640, 0.1164, 0.1349,  ..., 0.1004, 0.0979, 0.1062],\n",
            "          [0.0883, 0.1438, 0.1655,  ..., 0.1700, 0.1129, 0.0874],\n",
            "          [0.1336, 0.1316, 0.0761,  ..., 0.1751, 0.1587, 0.0759]],\n",
            "\n",
            "         [[0.1418, 0.1801, 0.0636,  ..., 0.0670, 0.1574, 0.1061],\n",
            "          [0.1219, 0.1238, 0.0983,  ..., 0.2050, 0.0727, 0.1601],\n",
            "          [0.1401, 0.1027, 0.0924,  ..., 0.1198, 0.1375, 0.1539],\n",
            "          ...,\n",
            "          [0.1034, 0.1523, 0.0923,  ..., 0.1077, 0.1413, 0.1575],\n",
            "          [0.1439, 0.1598, 0.1069,  ..., 0.1287, 0.0969, 0.1553],\n",
            "          [0.1281, 0.0957, 0.0751,  ..., 0.1810, 0.1343, 0.0643]],\n",
            "\n",
            "         [[0.0887, 0.1581, 0.1164,  ..., 0.1099, 0.1778, 0.1109],\n",
            "          [0.1233, 0.1140, 0.0985,  ..., 0.0614, 0.0862, 0.1438],\n",
            "          [0.0952, 0.0809, 0.2082,  ..., 0.1266, 0.1194, 0.0884],\n",
            "          ...,\n",
            "          [0.1378, 0.1271, 0.1535,  ..., 0.1829, 0.1473, 0.0863],\n",
            "          [0.1710, 0.1450, 0.1516,  ..., 0.0978, 0.1040, 0.1377],\n",
            "          [0.0823, 0.0833, 0.1807,  ..., 0.1237, 0.0926, 0.1938]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0883, 0.0875, 0.0924,  ..., 0.1407, 0.1071, 0.1081],\n",
            "          [0.0714, 0.0823, 0.0752,  ..., 0.1351, 0.1445, 0.1844],\n",
            "          [0.1050, 0.1974, 0.1106,  ..., 0.0571, 0.1099, 0.1213],\n",
            "          ...,\n",
            "          [0.1108, 0.1247, 0.1002,  ..., 0.1505, 0.1915, 0.1933],\n",
            "          [0.1155, 0.1108, 0.1455,  ..., 0.1601, 0.1204, 0.0946],\n",
            "          [0.0750, 0.1196, 0.1080,  ..., 0.0879, 0.2136, 0.0884]],\n",
            "\n",
            "         [[0.0883, 0.0875, 0.0924,  ..., 0.1407, 0.1071, 0.1081],\n",
            "          [0.0714, 0.0823, 0.0752,  ..., 0.1351, 0.1445, 0.1844],\n",
            "          [0.1050, 0.1974, 0.1106,  ..., 0.0571, 0.1099, 0.1213],\n",
            "          ...,\n",
            "          [0.1108, 0.1247, 0.1002,  ..., 0.1505, 0.1915, 0.1933],\n",
            "          [0.1155, 0.1108, 0.1455,  ..., 0.1601, 0.1204, 0.0946],\n",
            "          [0.0750, 0.1196, 0.1080,  ..., 0.0879, 0.2136, 0.0884]],\n",
            "\n",
            "         [[0.0883, 0.0875, 0.0924,  ..., 0.1407, 0.1071, 0.1081],\n",
            "          [0.0714, 0.0823, 0.0752,  ..., 0.1351, 0.1445, 0.1844],\n",
            "          [0.1050, 0.1974, 0.1106,  ..., 0.0571, 0.1099, 0.1213],\n",
            "          ...,\n",
            "          [0.1108, 0.1247, 0.1002,  ..., 0.1505, 0.1915, 0.1933],\n",
            "          [0.1155, 0.1108, 0.1455,  ..., 0.1601, 0.1204, 0.0946],\n",
            "          [0.0750, 0.1196, 0.1080,  ..., 0.0879, 0.2136, 0.0884]]],\n",
            "\n",
            "\n",
            "        [[[0.0650, 0.0863, 0.1072,  ..., 0.1120, 0.1493, 0.1271],\n",
            "          [0.1565, 0.1651, 0.0859,  ..., 0.1236, 0.1543, 0.1018],\n",
            "          [0.0857, 0.1215, 0.1449,  ..., 0.1119, 0.1423, 0.0870],\n",
            "          ...,\n",
            "          [0.0936, 0.1133, 0.0971,  ..., 0.0969, 0.1094, 0.2136],\n",
            "          [0.1395, 0.0973, 0.2096,  ..., 0.0805, 0.1650, 0.1237],\n",
            "          [0.0903, 0.1151, 0.2387,  ..., 0.0811, 0.1337, 0.1265]],\n",
            "\n",
            "         [[0.1418, 0.1801, 0.0636,  ..., 0.0670, 0.1574, 0.1061],\n",
            "          [0.1219, 0.1238, 0.0983,  ..., 0.2050, 0.0727, 0.1601],\n",
            "          [0.1401, 0.1027, 0.0924,  ..., 0.1198, 0.1375, 0.1539],\n",
            "          ...,\n",
            "          [0.1034, 0.1523, 0.0923,  ..., 0.1077, 0.1413, 0.1575],\n",
            "          [0.1439, 0.1598, 0.1069,  ..., 0.1287, 0.0969, 0.1553],\n",
            "          [0.1281, 0.0957, 0.0751,  ..., 0.1810, 0.1343, 0.0643]],\n",
            "\n",
            "         [[0.1613, 0.0716, 0.0971,  ..., 0.0663, 0.0843, 0.1609],\n",
            "          [0.0887, 0.2073, 0.1318,  ..., 0.1067, 0.1270, 0.1001],\n",
            "          [0.1870, 0.1022, 0.0802,  ..., 0.0875, 0.1001, 0.1372],\n",
            "          ...,\n",
            "          [0.2177, 0.1085, 0.1772,  ..., 0.0978, 0.1096, 0.0884],\n",
            "          [0.2453, 0.0948, 0.1714,  ..., 0.0962, 0.0534, 0.1131],\n",
            "          [0.1289, 0.1875, 0.0793,  ..., 0.1010, 0.0877, 0.0930]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0883, 0.0875, 0.0924,  ..., 0.1407, 0.1071, 0.1081],\n",
            "          [0.0714, 0.0823, 0.0752,  ..., 0.1351, 0.1445, 0.1844],\n",
            "          [0.1050, 0.1974, 0.1106,  ..., 0.0571, 0.1099, 0.1213],\n",
            "          ...,\n",
            "          [0.1108, 0.1247, 0.1002,  ..., 0.1505, 0.1915, 0.1933],\n",
            "          [0.1155, 0.1108, 0.1455,  ..., 0.1601, 0.1204, 0.0946],\n",
            "          [0.0750, 0.1196, 0.1080,  ..., 0.0879, 0.2136, 0.0884]],\n",
            "\n",
            "         [[0.0883, 0.0875, 0.0924,  ..., 0.1407, 0.1071, 0.1081],\n",
            "          [0.0714, 0.0823, 0.0752,  ..., 0.1351, 0.1445, 0.1844],\n",
            "          [0.1050, 0.1974, 0.1106,  ..., 0.0571, 0.1099, 0.1213],\n",
            "          ...,\n",
            "          [0.1108, 0.1247, 0.1002,  ..., 0.1505, 0.1915, 0.1933],\n",
            "          [0.1155, 0.1108, 0.1455,  ..., 0.1601, 0.1204, 0.0946],\n",
            "          [0.0750, 0.1196, 0.1080,  ..., 0.0879, 0.2136, 0.0884]],\n",
            "\n",
            "         [[0.0883, 0.0875, 0.0924,  ..., 0.1407, 0.1071, 0.1081],\n",
            "          [0.0714, 0.0823, 0.0752,  ..., 0.1351, 0.1445, 0.1844],\n",
            "          [0.1050, 0.1974, 0.1106,  ..., 0.0571, 0.1099, 0.1213],\n",
            "          ...,\n",
            "          [0.1108, 0.1247, 0.1002,  ..., 0.1505, 0.1915, 0.1933],\n",
            "          [0.1155, 0.1108, 0.1455,  ..., 0.1601, 0.1204, 0.0946],\n",
            "          [0.0750, 0.1196, 0.1080,  ..., 0.0879, 0.2136, 0.0884]]]],\n",
            "       grad_fn=<SoftmaxBackward>)\n",
            "torch.Size([10, 20, 8, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsaxqXTLcCx2",
        "outputId": "68b25b9c-d74a-4d4c-8a14-7ba3e0507e93"
      },
      "source": [
        "attn_values = torch.matmul(attn_dists, v)\n",
        "\n",
        "print(attn_values.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 20, 8, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue4_Kw3xcMT_"
      },
      "source": [
        "## Merge each result\n",
        "\n",
        "Concatenate each result.\n",
        "\n",
        "And add fully-connected layers(linear transformation) with the proper dimension."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijESc2Ffcf82",
        "outputId": "df4d5a96-2eea-4510-aa84-048ba9128486"
      },
      "source": [
        "attn_values = attn_values.transpose(1, 2) # (B, L, num_heads, d_k)\n",
        "attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  #(B, L, d_model)\n",
        "\n",
        "print(attn_values.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 20, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TnvrY01c_cl",
        "outputId": "c8aece68-848c-4639-d835-d5bdc01730d2"
      },
      "source": [
        "outputs = w_0(attn_values)\n",
        "\n",
        "print(outputs)\n",
        "print(outputs.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-4.2419e-02, -5.0413e-02, -7.5183e-03,  ..., -3.6984e-02,\n",
            "          -5.2476e-03, -2.6158e-02],\n",
            "         [ 5.7163e-02,  4.9516e-02,  1.2621e-02,  ...,  6.9614e-03,\n",
            "          -1.6063e-01, -1.2941e-01],\n",
            "         [ 1.3234e-01,  2.2052e-01, -2.5090e-02,  ..., -1.6492e-01,\n",
            "           7.6562e-02,  1.3833e-01],\n",
            "         ...,\n",
            "         [ 1.2553e-01,  1.9933e-01, -4.3863e-02,  ..., -1.7124e-01,\n",
            "           7.2022e-02,  1.5761e-01],\n",
            "         [-6.7493e-02,  1.6571e-01, -1.8425e-01,  ...,  2.4885e-01,\n",
            "           1.5054e-01,  5.1648e-02],\n",
            "         [-3.9699e-02,  8.8506e-02,  1.0955e-01,  ..., -8.0166e-02,\n",
            "           1.7501e-01, -1.9001e-01]],\n",
            "\n",
            "        [[-1.0307e-01,  4.8448e-02,  1.4954e-01,  ...,  2.3246e-01,\n",
            "           2.9636e-03,  1.3784e-01],\n",
            "         [ 8.2978e-02,  2.1908e-02,  6.0375e-02,  ..., -2.0790e-01,\n",
            "           1.2990e-01,  9.4210e-02],\n",
            "         [ 1.9119e-01, -2.3776e-02, -1.0227e-01,  ..., -7.8632e-02,\n",
            "           1.2237e-01,  1.1746e-01],\n",
            "         ...,\n",
            "         [ 9.8120e-02,  8.6619e-03, -3.1606e-02,  ..., -7.6797e-02,\n",
            "           2.0051e-01,  1.5393e-01],\n",
            "         [-4.8542e-02, -1.7765e-02,  1.2420e-01,  ..., -1.5219e-01,\n",
            "          -5.9377e-02, -1.1445e-01],\n",
            "         [-1.9721e-02,  1.3111e-02,  9.6278e-02,  ..., -1.8607e-01,\n",
            "           3.4218e-02,  5.0566e-02]],\n",
            "\n",
            "        [[-7.4650e-02, -8.0263e-02,  1.1614e-02,  ...,  1.0222e-01,\n",
            "           2.8773e-02, -9.4111e-02],\n",
            "         [-3.5199e-02, -3.0836e-04,  1.6520e-01,  ..., -6.4741e-02,\n",
            "           1.4399e-01, -1.2085e-01],\n",
            "         [ 3.6310e-02, -5.3646e-02,  1.5041e-02,  ..., -1.1860e-01,\n",
            "           7.7246e-02, -3.1808e-02],\n",
            "         ...,\n",
            "         [-2.8029e-02,  9.9294e-03,  4.1198e-02,  ..., -1.6686e-01,\n",
            "           5.0145e-02,  1.2579e-02],\n",
            "         [-3.0741e-02,  6.3265e-02, -6.2280e-02,  ...,  1.0711e-01,\n",
            "          -9.1786e-02,  1.7823e-01],\n",
            "         [-1.9721e-02,  1.3111e-02,  9.6278e-02,  ..., -1.8607e-01,\n",
            "           3.4218e-02,  5.0566e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 2.6095e-01,  2.6696e-01, -4.2897e-02,  ..., -6.4007e-02,\n",
            "          -1.6080e-01,  1.2346e-01],\n",
            "         [ 2.2340e-02,  8.1873e-02,  1.0584e-01,  ..., -2.2134e-01,\n",
            "          -1.9412e-01, -1.7814e-01],\n",
            "         [ 1.4040e-01,  1.9620e-03, -1.3487e-01,  ..., -1.3114e-01,\n",
            "          -1.4388e-01, -6.3338e-02],\n",
            "         ...,\n",
            "         [ 4.3356e-02,  4.1854e-02, -1.3500e-01,  ..., -1.8622e-01,\n",
            "          -1.8286e-01,  2.8131e-02],\n",
            "         [ 1.3154e-01, -9.7600e-02, -1.6328e-02,  ...,  1.7540e-01,\n",
            "          -2.5433e-01,  4.8954e-02],\n",
            "         [ 3.3234e-02, -1.6552e-01,  7.0977e-02,  ...,  9.4336e-02,\n",
            "           4.7684e-02, -6.8519e-02]],\n",
            "\n",
            "        [[ 9.2628e-02, -2.4994e-01,  2.0666e-01,  ..., -1.5123e-01,\n",
            "           1.4199e-01,  1.2230e-01],\n",
            "         [ 8.0644e-03, -5.9207e-02,  4.7012e-02,  ..., -2.2304e-02,\n",
            "          -1.6260e-01, -7.1999e-02],\n",
            "         [-7.5521e-03, -1.3512e-01, -1.7551e-02,  ..., -1.5055e-01,\n",
            "           2.7966e-02,  4.5737e-02],\n",
            "         ...,\n",
            "         [-1.5020e-02, -7.2117e-02, -6.0053e-03,  ..., -1.6716e-01,\n",
            "           3.4833e-02,  6.9927e-02],\n",
            "         [ 7.6161e-02,  2.8748e-02,  7.7995e-02,  ..., -3.2656e-02,\n",
            "           1.3336e-01,  1.3016e-01],\n",
            "         [-1.5052e-01,  5.8053e-02,  1.2131e-02,  ..., -1.2266e-01,\n",
            "           1.2885e-02, -1.0408e-01]],\n",
            "\n",
            "        [[-1.5728e-02,  4.8487e-02,  5.4232e-02,  ..., -1.9491e-03,\n",
            "           6.1307e-02,  7.3058e-02],\n",
            "         [ 2.1015e-01, -3.3290e-02, -9.5274e-02,  ...,  8.0291e-02,\n",
            "          -1.8843e-01,  5.2853e-02],\n",
            "         [ 8.8006e-02, -1.2373e-01, -1.1190e-02,  ..., -2.0380e-01,\n",
            "           1.0585e-01, -1.0210e-02],\n",
            "         ...,\n",
            "         [ 1.7098e-02, -1.4006e-01, -3.1875e-02,  ..., -1.2716e-01,\n",
            "           9.5921e-02, -4.0559e-02],\n",
            "         [-1.0724e-01, -2.1606e-02, -4.7547e-02,  ..., -1.0971e-01,\n",
            "          -1.3205e-02, -6.0487e-02],\n",
            "         [ 4.8796e-02,  5.6884e-05,  1.4889e-01,  ..., -4.1243e-02,\n",
            "          -1.2375e-01,  1.5872e-02]]], grad_fn=<AddBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JLaTe7ndFJ1"
      },
      "source": [
        "## The Whole Code\n",
        "\n",
        "Put the codes above together to make Multi-head attention module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28ge8Lc1dTuM"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "    # Q, K, V learnerable matrices\n",
        "    self.w_q = nn.Linear(d_model, d_model)\n",
        "    self.w_k = nn.Linear(d_model, d_model)\n",
        "    self.w_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "    # Linear transformation for concatenated outputs\n",
        "    self.w_0 = nn.Linear(d_model, d_model)\n",
        "\n",
        "  def forward(self, q, k, v):\n",
        "    batch_size = q.shape[0]\n",
        "\n",
        "    q = self.w_q(q) # (B, L, d_model)\n",
        "    k = self.w_q(k) # (B, L, d_model)\n",
        "    v = self.w_q(v) # (B, L, d_model)\n",
        "\n",
        "    q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "    k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "    v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "\n",
        "    q = q.transpose(1, 2) # (B, num_heads, L, d_k)\n",
        "    k = k.transpose(1, 2) # (B, num_heads, L, d_k)\n",
        "    v = v.transpose(1, 2) # (B, num_heads, L, d_k)\n",
        "\n",
        "    attn_values = self.self_attention(q, k, v)\n",
        "    attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, d_model)  # (B, L, num_heads, d_k) => (B, L, d_model)\n",
        "\n",
        "    return self.w_0(attn_values)\n",
        "  \n",
        "  def self_attention(self, q, k, v):\n",
        "    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k) # (B, num_heads, L, L)\n",
        "    attn_dists = F.softmax(attn_scores, dim = -1) # (B, num_heads, L, L)\n",
        "\n",
        "    attn_values = torch.matmul(attn_dists, v) # (B, num_heads, L, d_k)\n",
        "\n",
        "    return attn_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2yUnisJfjQh"
      },
      "source": [
        "multihead_attn = MultiHeadAttention()\n",
        "\n",
        "outputs = multihead_attn(batch_emb, batch_emb, batch_emb) # (B, L, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzFmVjUWfwSx",
        "outputId": "59a5680c-27c7-43bf-8dc7-851ae4bc880d"
      },
      "source": [
        "print(outputs)\n",
        "print(outputs.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-0.0953, -0.2668, -0.0293,  ...,  0.1291, -0.0275, -0.2341],\n",
            "         [-0.5509,  0.0514,  0.0039,  ...,  0.0297,  0.0797,  0.1726],\n",
            "         [-0.3558,  0.0457,  0.0024,  ...,  0.1884,  0.3044,  0.2683],\n",
            "         ...,\n",
            "         [-0.0263, -0.2028, -0.0705,  ...,  0.1075,  0.2533, -0.2356],\n",
            "         [-0.0263, -0.2028, -0.0705,  ...,  0.1075,  0.2533, -0.2356],\n",
            "         [-0.0263, -0.2028, -0.0705,  ...,  0.1075,  0.2533, -0.2356]],\n",
            "\n",
            "        [[-0.1758,  0.0447, -0.1223,  ...,  0.0823,  0.2819,  0.0240],\n",
            "         [ 0.0085, -0.3409, -0.1536,  ..., -0.0007,  0.3156,  0.1716],\n",
            "         [-0.0534, -0.0326, -0.0608,  ...,  0.2224,  0.1670, -0.1612],\n",
            "         ...,\n",
            "         [-0.0351, -0.2495, -0.0925,  ...,  0.1341,  0.3069, -0.3234],\n",
            "         [-0.0351, -0.2495, -0.0925,  ...,  0.1341,  0.3069, -0.3234],\n",
            "         [-0.0351, -0.2495, -0.0925,  ...,  0.1341,  0.3069, -0.3234]],\n",
            "\n",
            "        [[ 0.2885, -0.0145, -0.0887,  ...,  0.1108,  0.2228, -0.2776],\n",
            "         [ 0.2202,  0.0567, -0.2858,  ...,  0.3749,  0.3151, -0.1270],\n",
            "         [ 0.0637,  0.2252, -0.1321,  ..., -0.0050,  0.1205, -0.4592],\n",
            "         ...,\n",
            "         [-0.0524, -0.2124, -0.0809,  ...,  0.1237,  0.3246, -0.3047],\n",
            "         [-0.0524, -0.2124, -0.0809,  ...,  0.1237,  0.3246, -0.3047],\n",
            "         [-0.0524, -0.2124, -0.0809,  ...,  0.1237,  0.3246, -0.3047]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0380,  0.0733,  0.1933,  ...,  0.1936, -0.4109, -0.3890],\n",
            "         [-0.0944,  0.0102, -0.0722,  ...,  0.1178,  0.1370, -0.2374],\n",
            "         [ 0.0014,  0.0646,  0.0931,  ...,  0.2911, -0.1208, -0.1104],\n",
            "         ...,\n",
            "         [-0.0655,  0.2093,  0.3076,  ...,  0.0679, -0.1262, -0.1047],\n",
            "         [ 0.0091,  0.0505,  0.1214,  ...,  0.1403, -0.0601, -0.4223],\n",
            "         [-0.1146,  0.1443,  0.1425,  ...,  0.1182,  0.0591, -0.1563]],\n",
            "\n",
            "        [[-0.1741,  0.0765,  0.1727,  ..., -0.1165,  0.1799, -0.1340],\n",
            "         [-0.1673,  0.1327, -0.3421,  ...,  0.1408,  0.1243, -0.1595],\n",
            "         [-0.1119,  0.1588, -0.1352,  ...,  0.0790,  0.0826, -0.3594],\n",
            "         ...,\n",
            "         [-0.1012, -0.1480, -0.0915,  ...,  0.1132,  0.2962, -0.2838],\n",
            "         [-0.1012, -0.1480, -0.0915,  ...,  0.1132,  0.2962, -0.2838],\n",
            "         [-0.1012, -0.1480, -0.0915,  ...,  0.1132,  0.2962, -0.2838]],\n",
            "\n",
            "        [[ 0.2485, -0.1537, -0.1792,  ..., -0.2285, -0.0705, -0.3016],\n",
            "         [-0.0557,  0.0643, -0.3059,  ...,  0.1327,  0.0397, -0.1406],\n",
            "         [-0.0852, -0.1163, -0.0906,  ...,  0.0193, -0.0191,  0.1189],\n",
            "         ...,\n",
            "         [-0.0160, -0.2067, -0.0640,  ...,  0.1220,  0.2729, -0.2949],\n",
            "         [-0.0160, -0.2067, -0.0640,  ...,  0.1220,  0.2729, -0.2949],\n",
            "         [-0.0160, -0.2067, -0.0640,  ...,  0.1220,  0.2729, -0.2949]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qbMzvUPf_A9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}